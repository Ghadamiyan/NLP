{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1   \n",
    "Choose a wikipedia article. You will download and acces the article using this python module: wikipedia. Use the content property to extract the text. Print the title of the article and the first N=200 words from the article to verify that all works well. Print the POS-tagging for the first N=20 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in d:\\anaconda\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.page(\"Riaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recording Industry Association of America'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Recording_Industry_Association_of_America'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Recording Industry Association of America (RIAA) is a trade organization that represents the recording industry in the United States. Its members consist of record labels and distributors, which t'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data = sent_tokenize(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './postagger/models/english-bidirectional-distsim.tagger'\n",
    "jar_tagger_path = './postagger/stanford-postagger-4.2.0.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = StanfordPOSTagger(model_path, jar_tagger_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number 1 :\n",
      "\n",
      "[('The', 'DT'), ('Recording', 'NNP'), ('Industry', 'NNP'), ('Association', 'NNP'), ('of', 'IN'), ('America', 'NNP'), ('(RIAA)', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('trade', 'NN'), ('organization', 'NN'), ('that', 'WDT'), ('represents', 'VBZ'), ('the', 'DT'), ('recording', 'NN'), ('industry', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States.', 'NNP')]\n",
      "\n",
      "\n",
      "Sentence number 2 :\n",
      "\n",
      "[('Its', 'PRP$'), ('members', 'NNS'), ('consist', 'VBP'), ('of', 'IN'), ('record', 'NN'), ('labels', 'NNS'), ('and', 'CC'), ('distributors,', 'NN'), ('which', 'WDT'), ('the', 'DT'), ('RIAA', 'NNP'), ('says', 'VBZ'), ('\"create,', 'FW'), ('manufacture,', 'FW'), ('and/or', 'CC'), ('distribute', 'VBP'), ('approximately', 'RB'), ('85%', 'CD'), ('of', 'IN'), ('all', 'DT'), ('legally', 'RB'), ('sold', 'VBN'), ('recorded', 'JJ'), ('music', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States\".', 'NNP')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "for i in range(N):\n",
    "    sentence = tok_data[i]\n",
    "    pos_tokens = tagger.tag(sentence.split()) \n",
    "    print('Sentence number '+str(i+1)+' :\\n')\n",
    "    print(pos_tokens)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2    \n",
    "Create a function that receives a part of speech tag and returns a list with all the words from the text (can be given as a parameter too) that represent that part of speech. Create a function that receives a list of POS tags and returns a list with words having any of the given POS tags (use the first function in implementing the second one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "    data1 = data.translate(translate_table)\n",
    "    data2 = word_tokenize(data1)\n",
    "\n",
    "    data2 = [w.lower() for w in data2]\n",
    "\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    preprocessed_data = [x for x in data2 if not x in stopwords]  \n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_pos(pos_tag, text):\n",
    "    words = []\n",
    "    pp_text = preprocessing(text)\n",
    "    pos_tokens = tagger.tag(pp_text)\n",
    "    for tuple_ in pos_tokens:\n",
    "        if tuple_[1] == pos_tag:\n",
    "            words.append(tuple_[0])  \n",
    "            #words.append(tuple_)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recording', 'industry', 'association', 'trade', 'organization', 'industry']\n"
     ]
    }
   ],
   "source": [
    "pos_tag = 'NN'\n",
    "text = 'The Recording Industry Association of America (RIAA) is a trade organization that represents the recording industry in the United States.'\n",
    "\n",
    "lst = all_words_pos(pos_tag, text)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_multiple_pos(pos_tags, text):\n",
    "    lst = []\n",
    "    for pos_tag in pos_tags:\n",
    "        lst.append(all_words_pos(pos_tag, text))\n",
    "    \n",
    "    lst = [word for sublist in lst for word in sublist]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recording', 'industry', 'association', 'trade', 'organization', 'industry', 'represents']\n"
     ]
    }
   ],
   "source": [
    "pos_tags = ['NN', 'VBZ']\n",
    "text = 'The Recording Industry Association of America (RIAA) is a trade organization that represents the recording industry in the United States.'\n",
    "\n",
    "lst = all_words_multiple_pos(pos_tags, text)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3\n",
    "Use the function above to print all the nouns (there are multiple tags for nouns), and, respectively all the verbs (corresponding to all verb tags). Also, print the percentage of content words (noun+verbs) from the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['riaa', 'internet', 'web', 'site', 'p2plawsuitscom', 'discount', 'credit', 'card42', 'anyone', 'strategy', 'income', 'settlements43', 'riaa', 'settlement', 'program', 'pass', 'disclosure', 'settlement', 'evidence', 'benefit', 'riaa', 'riaa', 'website', 'purpose', 'discount', 'settlement', 'credit', 'card45', 'march', 'focus', 'riaa', 'lawsuit', 'ninth', 'court', 'diamond', 'player', 'home', 'recording', 'act', 'consumer', 'mp3', 'audio', 'player', 'market', 'threejudge', 'panel', 'favor', 'diamond', 'way', 'development', 'player', 'market', 'riaa', 'college', 'student', 'search', 'network', 'music', 'thievery', 'riaa', 'suit', 'court', 'kazaa', 'publisher', 'sharman', 'lawsuit', 'riaa', 'network', 'client', 'software', 'investigation', 'track', 'file', 'effort', 'throw', 'suit', 'suit', 'sharman', 'settlement', 'litigation', 'motion', 'picture', 'association', 'america', 'federation', 'industry', 'riaa', 'network', 'riaa', 'mpaa', 'software', 'industry', 'network', 'riaa', 'suit', 'xm', 'satellite', 'radio', 'satellite', 'broadcasts53', 'internet', 'radio', 'riaa', 'usenetcom', 'injunction', 'company', 'copyright', 'infringement', 'suit', 'riaa', 'usenet', 'provider', 'branch', 'riaa', 'fight', 'curb', 'distribution', 'riaa', 'provider', 'service', 'content', 'riaa', 'argument', 'fact', 'usenetcom', 'defendant', 'service', 'service', 'music', 'riaa', 'member', 'project', 'playlist', 'web', 'music', 'search', 'site', 'site', 'index', 'project', 'playlist', 'website', 'music', 'project', 'riaa', 'fight', 'usenetcom', 'decision', 'district', 'judge', 'district', 'favor', 'music', 'industry', 'infringement', 'addition', 'baer', 'claim', 'protection', 'decision', 'ruling', 'infringement', 'device', 'noninfringing', 'uses56', 'court', 'damage', 'music', 'industry', 'riaa', 'case', 'network', 'distribution', 'works58', 'retaliation', 'riaaorg', 'denialofservice', 'operation', 'payback', 'letters', 'users', 'files', 'settlements', 'letters', 'lawsuits', 'settlements', 'fees', 'ciara', 'songs', 'isps', 'colleges', 'universities', 'letters', 'subscribers', 'students', 'settlements', 'identities', 'letters', 'isps', 'students', 'subscribers', 'isps', 'colleges', 'universities', 'appeals', 'multimedia', 'developers', 'engines', 'individuals', 'numbers', 'files', 'suits', 'payments', 'networks', 'terms', 'sharers', 'networks', 'creators', 'amounts', 'filters', 'networks', 'users', 'works', 'subscribers', 'songs', 'stations', 'materials', 'lawsuits', 'means', 'slogans', 'phrases', 'labels', 'recordings', 'links', 'playlists', 'servers', 'arguments', 'cases', 'states', 'companies', 'assessments', 'awards', 'millions', 'dollars', 'members', 'attacks', 'members', 'february', 'october', 'lan', 'september', 'january', 'october', 'april', 'june', 'harold', 'baer', 'york', 'sony', 'betamax', 'october', 'limewire', 'p2p', 'october']\n"
     ]
    }
   ],
   "source": [
    "pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "text = 'In February 2007, the RIAA began sending letters accusing Internet users of sharing files and directing them to web site P2PLAWSUITS.COM, where they can make discount settlements payable by credit card.[42] The letters go on to say that anyone not settling will have lawsuits brought against them. Typical settlements are between $3,000 and $12,000. This new strategy was formed because the RIAA legal fees were cutting into the income from settlements.[43] In 2008, RIAA sued 19-year-old Ciara Sauro for allegedly sharing 10 songs online. The RIAA also launched an early settlement program directed to ISPs and to colleges and universities, urging them to pass along letters to subscribers and students offering early settlements, prior to the disclosure of their identities. The settlement letters urged ISPs to preserve evidence for the benefit of the RIAA and invited the students and subscribers to visit an RIAA website for the purpose of entering into a discount settlement payable by credit card.[45] By March 2007, the focus had shifted from ISPs to colleges and universities. In October 1998, the RIAA filed a lawsuit in the Ninth U.S. Court of Appeals in San Francisco claiming the Diamond Multimedia Rio PMP300 player violated the 1992 Audio Home Recording Act. The Rio PMP300 was significant because it was the second portable consumer MP3 digital audio player released on the market. The three-judge panel ruled in favor of Diamond, paving the way for the development of the portable digital player market. In 2003, the RIAA sued college student developers of LAN search engines Phynd and Flatlan, describing them as a sophisticated network designed to enable widespread music thievery. In September 2003, the RIAA filed suit in civil court against several private individuals who had shared large numbers of files with Kazaa. Most of these suits were settled with monetary payments averaging 3,000. Kazaa publisher Sharman Networks responded with a lawsuit against the RIAA, alleging that the terms of use of the network were violated and that unauthorized client software was used in the investigation to track down the individual file sharers (such as Kazaa Lite). An effort to throw out this suit was denied in January 2004, but that suit was settled in 2006. Sharman Networks agreed to a global settlement of litigation brought against it by the Motion Picture Association of America, the International Federation of the Phonographic Industry, and the RIAA. The creators of the popular Kazaa file-sharing network will pay $115 million to the RIAA, unspecified future amounts to the MPAA and the software industry, and install filters on its networks to prevent users from sharing copyrighted works on its network. RIAA has also filed suit in 2006 to enjoin digital XM Satellite Radio from enabling its subscribers from playing songs it has recorded from its satellite broadcasts.[53] It is also suing several Internet radio stations. On October 12, 2007, the RIAA sued Usenet.com seeking a permanent injunction to prevent the company from aiding, encouraging, enabling, inducing, causing, materially contributing to, or otherwise facilitating copyright infringement. This suit, the first that the RIAA has filed against a Usenet provider, has added another branch to the RIAA rapidly expanding fight to curb the unauthorized distribution of copyrighted materials. Unlike many of the RIAA previous lawsuits, this suit is filed against the provider of a service who has no direct means of removing infringing content. The RIAA argument relies heavily on the fact the Usenet.com, the only defendant that has been named currently, promoted their service with slogans and phrases that strongly suggested that the service could be used to obtain free music. On April 28, 2008, RIAA member labels sued Project Playlist, a web music search site, claiming that most of the sound recordings in the site index of links are infringing. Project Playlist website denies that any of the music is hosted on Project Playlists own servers. On June 30, 2009, the RIAA prevailed in its fight against Usenet.com, in a decision, that the U.S. District Judge Harold Baer of the Southern District of New York ruled in favor of the music industry on all its main arguments: that Usenet.com is guilty of direct, contributory, and vicarious infringement. In addition, and perhaps most importantly for future cases, Baer said that Usenet.com cannot claim protection under the Sony Betamax decision. That ruling states that companies cannot be held liable for contributory infringement if the device they create is \"capable of significant noninfringing uses\".[56] Furthermore, the parties are now headed to federal court for damage assessments and awards, which could amount to several millions of dollars for the music industry. On October 26, 2010, RIAA members won a case against LimeWire, a P2P file-sharing network, for illegal distribution of copyrighted works.[58] On October 29, in retaliation, riaa.org was taken offline via denial-of-service attacks executed by members of Operation Payback and Anonymous.'\n",
    "\n",
    "lst_nouns = all_words_multiple_pos(pos_tags, text)\n",
    "print(lst_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'preserve', 'pay', 'install', 'obtain', 'amount', 'began', 'brought', 'sued', 'urged', 'invited', 'shifted', 'violated', 'ruled', 'sued', 'filed', 'shared', 'settled', 'responded', 'violated', 'denied', 'settled', 'agreed', 'brought', 'filed', 'recorded', 'sued', 'added', 'suggested', 'sued', 'prevailed', 'said', 'held', 'executed', 'sending', 'accusing', 'sharing', 'directing', 'settling', 'cutting', 'sharing', 'urging', 'offering', 'entering', 'claiming', 'paving', 'describing', 'averaging', 'alleging', 'sharing', 'enabling', 'playing', 'suing', 'seeking', 'aiding', 'encouraging', 'inducing', 'causing', 'contributing', 'facilitating', 'expanding', 'removing', 'infringing', 'claiming', 'infringing', 'filesharing', 'formed', 'launched', 'directed', 'filed', 'released', 'designed', 'used', 'copyrighted', 'filed', 'copyrighted', 'filed', 'named', 'promoted', 'used', 'hosted', 'ruled', 'copyrighted', 'taken', 'make', 'go', 'sauro', 'riaa', 'visit', 'enable', 'kazaa', 'use', 'prevent', 'enjoin', 'prevent', 'suit', 'usenetcom', 'create', 'relies', 'denies', 'parties']\n"
     ]
    }
   ],
   "source": [
    "pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "lst_vbs = all_words_multiple_pos(pos_tags, text)\n",
    "print(lst_vbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of content words is: 0.4539800995024876\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of content words is: '+str((len(lst_nouns)+len(lst_vbs))/len(text.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4\n",
    "Print a table of four columns. The columns will be separated with the character \"|\". The head of the table will be:\n",
    "Original word | POS | Simple lemmatization | Lemmatization with POS\n",
    "that will compare the results of lemmatization (WordNetLemmatizer) without giving the part of speech and the lemmatization with the given part of speech for each word. The table must contain only words that give different results for the two lemmatizations (for example, the word \"running\" - without POS, the result will always be running, but with pos=\"v\" it is \"run\"). The table will contain the results for the first N sentences from the text (each row corresponding to a word). Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "words = []\n",
    "for contor, sentence in enumerate(tok_data):\n",
    "    if contor <= N:\n",
    "        \n",
    "        from nltk.corpus import stopwords\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        sentence = sentence.translate(translate_table)\n",
    "        \n",
    "        words_ = word_tokenize(sentence)\n",
    "        \n",
    "        words_ = [w.lower() for w in words_]\n",
    "        \n",
    "        stopwords = set(stopwords.words('english'))\n",
    "        words_ = [x for x in words_ if not x in stopwords]  \n",
    "        \n",
    "        for word in words_:\n",
    "            words.append(word)\n",
    "\n",
    "originals = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for word in originals:\n",
    "    lemmas.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = tagger.tag(originals)\n",
    "pos = []\n",
    "for word, tag in pos_:\n",
    "    pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/46231553/12299607\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemmas = []\n",
    "for word, tag in pos_:\n",
    "    word_net_tag = get_wordnet_pos(tag)\n",
    "    \n",
    "    if word_net_tag is None:\n",
    "        pos_lemmas.append(lemmatizer.lemmatize(word))\n",
    "    else:\n",
    "        pos_lemmas.append(lemmatizer.lemmatize(word, word_net_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n",
      "43\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(originals))\n",
    "print(len(lemmas))\n",
    "print(len(pos))\n",
    "print(len(pos_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ = pd.DataFrame({'Original Word': originals, 'POS': pos, 'Simple Lemmatization': lemmas, 'Lemmatization with POS' : pos_lemmas} ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_dif = []\n",
    "pos_dif = [] \n",
    "lemmas_dif = []\n",
    "pos_lemmas_dif = []\n",
    "\n",
    "for index, row in table_.iterrows():\n",
    "    if row['Simple Lemmatization'] != row['Lemmatization with POS']:\n",
    "        originals_dif.append(row['Original Word'])\n",
    "        pos_dif.append(row['POS'])\n",
    "        lemmas_dif.append(row['Simple Lemmatization'])\n",
    "        pos_lemmas_dif.append(row['Lemmatization with POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original Word  POS Simple Lemmatization Lemmatization with POS\n",
      "0     recording  VBG            recording                 record\n",
      "1          sold  VBD                 sold                   sell\n",
      "2    represents  VBZ           represents              represent\n",
      "3        united  VBN               united                  unite\n",
      "4      relating  VBG             relating                 relate\n",
      "5        formed  VBN               formed                   form\n",
      "6      recorded  VBD             recorded                 record\n"
     ]
    }
   ],
   "source": [
    "table = pd.DataFrame({'Original Word': originals_dif, 'POS': pos_dif, 'Simple Lemmatization': lemmas_dif, 'Lemmatization with POS' : pos_lemmas_dif} ) \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5\n",
    "Print a graphic showing the number of words for each part of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN - 15\n",
      "NNS - 7\n",
      "VBZ - 4\n",
      "VBP - 3\n",
      "VB - 2\n",
      "VBG - 2\n",
      "CD - 2\n",
      "VBD - 2\n",
      "JJ - 2\n",
      "RB - 2\n",
      "VBN - 2\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "for w, f in nltk.FreqDist(pos).most_common(N):\n",
    "    print(str(w) + ' - ' + str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/35603850/12299607\n",
    "\n",
    "counts = Counter(pos)\n",
    "\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6\n",
    "Create your own grammar with different terminal symbols. Apply recursive descent parsing on a sentence with at least 5 different parts of speech and a tree of at least level 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRPS My) (NN dog))\n",
      "  (VP\n",
      "    (VBP has)\n",
      "    (NP (DT the) (ADJP (RBS most) (JJ beautiful)) (NNS eyes))))\n"
     ]
    }
   ],
   "source": [
    "gram=nltk.CFG.fromstring(\"\"\"  S -> NP VP \n",
    "NP -> PRPS NN\n",
    "VP -> VBZ ADJP | VBP NP \n",
    "PRPS -> \"My\" | \"Our\" | \"Your\"\n",
    "NN -> \"dog\" | \"cat\" | \"pet\" | \"homework\"\n",
    "VBP -> \"has\" \n",
    "NP -> DT ADJP NNS \n",
    "DT -> \"the\"\n",
    "ADJP -> JJ | RBS JJ\n",
    "NNS -> \"eyes\" \n",
    "RBS -> \"most\"\n",
    "JJ -> \"beautiful\" | \"playful\" | \"enjoyable\"\n",
    "VBZ -> \"is\" \"\"\")\n",
    "\n",
    "sent = [\"My\", \"dog\", \"has\", \"the\", \"most\", \"beautiful\", \"eyes\"]\n",
    "rdp = nltk.RecursiveDescentParser(gram)\n",
    "for tree in rdp.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7\n",
    "Apply shift reduce parsing on the same sentence and check programatically if the two trees are equal. Find a sentence with equal trees and a sentence with different results (we consider the tree different even when it has no sollution for one of the parsers, but has for the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence with different results\n",
    "\n",
    "srp = nltk.ShiftReduceParser(gram)\n",
    "for tree in srp.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (PRPS My) (NN homework)) (VP (VBZ is) (ADJP (JJ enjoyable))))\n",
      "\n",
      "\n",
      "\n",
      "(S (NP (PRPS My) (NN homework)) (VP (VBZ is) (ADJP (JJ enjoyable))))\n"
     ]
    }
   ],
   "source": [
    "# Sentence with the same results\n",
    "\n",
    "sent = [\"My\", \"homework\", \"is\", \"enjoyable\"]\n",
    "for tree in rdp.parse(sent):\n",
    "    print(tree)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "for tree in srp.parse(sent):\n",
    "    print(tree)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.app.srparser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
