{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXzmYVNAf_iF"
   },
   "source": [
    "# Lab 1 \n",
    "\n",
    " Ghadamiyan Lida 407 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MajBtM9rgLpC"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApTitrUBBj8I",
    "outputId": "1cd3a407-83a3-4c76-8d23-f4a8e82992d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuevGiWS9BlW",
    "outputId": "dd83a6dd-8177-4858-97c9-ee881b78b7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13709 sha256=d44a347025e30f387f6cb36ee9dcb9775dccab10f3cdf50fdaeec06ac6d174e3\n",
      "  Stored in directory: c:\\users\\acasa-pc\\appdata\\local\\pip\\cache\\wheels\\72\\b0\\3f\\1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hjX9nPSGGMk7"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numbers\n",
    "from num2words import num2words\n",
    "import collections\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "import collections\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.request import urlopen\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unQslzmQG-2G",
    "outputId": "e9e38552-9937-4b19-a198-52b2e1883e1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ACASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ACASA-PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5M75PnHUilo"
   },
   "source": [
    "# 1.\n",
    "Download it through python (inside the code, so you don't have to upload the file too when you send the solution for this exercise) with urlopen() from module urllib and read the entire text in one single string. If the download takes too much time at each running, download the file, but leave the former instructions in a comment (to show that you know how to access an online file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K9AFZjKvHDjV"
   },
   "outputs": [],
   "source": [
    "data = urlopen(\"https://www.gutenberg.org/files/64608/64608-0.txt\").read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0-zUQHHUqpO"
   },
   "source": [
    "# 2\n",
    "Remove the header (keep only the text starting from the title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j8WIlOymTTBJ"
   },
   "outputs": [],
   "source": [
    "data = data.split('263')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87VwImaJUtkS"
   },
   "source": [
    "# 3\n",
    "Print the number of sentences in the text. Print the average length (number of words) of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npGDpNIjR0nu",
    "outputId": "176eacae-6cbc-4b91-f614-a6d07effb019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences in the corpus: \n",
      "3478\n"
     ]
    }
   ],
   "source": [
    "data1 = data\n",
    "tok_data = sent_tokenize(data1)\n",
    "ns = len(tok_data)\n",
    "\n",
    "print(\"No of sentences in the corpus: \")\n",
    "print(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh-9B4SnS8tR",
    "outputId": "558f8d47-b733-4941-d1e9-e9b05cbbae82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average no of words per sentence:\n",
      "21.510350776308222\n"
     ]
    }
   ],
   "source": [
    "data2 = data\n",
    "words = word_tokenize(data2)\n",
    "\n",
    "print(\"Average no of words per sentence:\")\n",
    "print(len(words)/ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmYrVVImWvLb"
   },
   "source": [
    "# 4\n",
    "Find the collocations in the text (bigram and trigram). Use the nltk.collocations module You will print them only once not each time they appear.\n",
    "\n",
    "https://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mL66P5KZj-A"
   },
   "source": [
    "before cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9iu4rZ_SYuCG"
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "scored = finder.score_ngrams( bigrams.likelihood_ratio  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joBYeGbEZgnI",
    "outputId": "196c5666-a64e-4e18-9f41-0566f8444948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('’', 't'), 4277.3450807717), (('’', 's'), 3962.008927012769), (('.', '“'), 3182.2664211816973), (('”', '“'), 2136.1754960933595), (('an', '’'), 2081.777163084429), (('?', '”'), 1653.8802797705978), (('’', 'd'), 1607.029665227435), (('o', '’'), 1307.8915602926197), (('’', 'll'), 1091.163623124117), (('don', '’'), 1074.0803773578352)]\n"
     ]
    }
   ],
   "source": [
    "print(scored[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaBr7kcKjuX-",
    "outputId": "30ff705c-912f-4794-ef0f-bfb3b19507bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('don', '’', 't'), 9079.389293580018), (('ain', '’', 't'), 8693.515344231952), (('didn', '’', 't'), 7261.273918587527), (('’', '’', 't'), 7127.41051792534), (('can', '’', 't'), 7053.460240516909), (('won', '’', 't'), 7040.382324996055), (('’', 't', '’'), 6762.193421517844), (('wouldn', '’', 't'), 6749.249316142836), (('Don', '’', 't'), 6693.54753637191), (('’', 't', 'know'), 6670.148746721734)]\n"
     ]
    }
   ],
   "source": [
    "trigram = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "scored = finder.score_ngrams( trigram.likelihood_ratio )\n",
    "print(scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-keeJR1aEZp"
   },
   "source": [
    "after cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVS4wyJ0kW6M"
   },
   "source": [
    "# 5\n",
    "Create a list of all the words (in lower case) from the text, without the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jVdJ2twtZtfx"
   },
   "outputs": [],
   "source": [
    "translate_table = dict((ord(char), None) for char in string.punctuation)  # removing punctuation  \n",
    "new_data = data.translate(translate_table)\n",
    "\n",
    "new_data = new_data.replace('“','')   # removing additional symbols that are not included in string.punctuation\n",
    "new_data = new_data.replace('”','')\n",
    "new_data = new_data.replace('’','')\n",
    "\n",
    "words_ = word_tokenize(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e00ZnULnkpl9",
    "outputId": "e1689a97-6169-4939-ad68-999e29331bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'flower', 'that', 'grew', 'in', 'the', 'sand', 'the', 'flower', 'that']\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "for w in words_:\n",
    "    text.append(w.lower())\n",
    "print(text[:10]) # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rS7CeWXOaIoj"
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "scored = finder.score_ngrams( bigrams.likelihood_ratio  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BMdOrSWaKxp",
    "outputId": "ce21413f-2b58-46a1-ba86-26c1d4f28536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('project', 'gutenbergtm'), 776.1637187183584), (('i', 'guess'), 687.5349853689522), (('there', 'was'), 584.5586159564414), (('she', 'said'), 498.3436462104727), (('i', 'dont'), 469.0181901731668), (('with', 'a'), 413.22258872047917), (('did', 'not'), 402.54742189232616), (('had', 'been'), 393.8475661359369), (('of', 'the'), 389.08706768946274), (('in', 'the'), 374.5225795062453)]\n"
     ]
    }
   ],
   "source": [
    "print(scored[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRbIurLjkMpd",
    "outputId": "4a091f54-ca49-4cf4-ebfd-9a87de6af3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('project', 'gutenbergtm', 'electronic'), 1495.3609956180699), (('project', 'gutenbergtm', 'license'), 1330.6038951093688), (('well', 'i', 'guess'), 1307.0136179262042), (('the', 'project', 'gutenbergtm'), 1301.380617894003), (('there', 'was', 'a'), 1294.583537409537), (('full', 'project', 'gutenbergtm'), 1246.4984300373176), (('project', 'gutenbergtm', 'trademark'), 1236.8248083723279), (('project', 'gutenbergtm', 'works'), 1226.0384732819928), (('project', 'gutenbergtm', 'work'), 1216.8579253287326), (('project', 'gutenbergtm', 'ebooks'), 1208.4706509146)]\n"
     ]
    }
   ],
   "source": [
    "trigram = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(text)\n",
    "scored = finder.score_ngrams( trigram.likelihood_ratio )\n",
    "print(scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPXpwy0xl0Dd"
   },
   "source": [
    "# 6\n",
    "\n",
    "Print the first N most frequent words (alphanumeric strings) together with their number of appearances.\n",
    "\n",
    "https://www.nltk.org/book/ch01.html - 3.1 Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OxUi8_sl2Vd",
    "outputId": "5b86b655-5b44-46c0-93e1-3520ae40888e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the - 2511\n",
      "to - 1401\n",
      "a - 1382\n",
      "and - 1242\n",
      "her - 1208\n",
      "i - 1000\n",
      "she - 945\n",
      "of - 909\n",
      "you - 829\n",
      "in - 719\n",
      "it - 640\n",
      "an - 629\n",
      "was - 602\n",
      "that - 560\n",
      "with - 559\n"
     ]
    }
   ],
   "source": [
    "# Obs: The most common words are stop words, so we would have a more relevant result without them   \n",
    "\n",
    "N = 15\n",
    "for w, f in nltk.FreqDist(text).most_common(N):\n",
    "    print(str(w) + ' - ' + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8kqm1NOxkoi"
   },
   "source": [
    "# 7\n",
    "\n",
    "Remove stopwords and assign the result to variable lws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PkCpC8Anyd48"
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "lws = [x for x in text if not x in stopwords]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtDNqtlgzdGd",
    "outputId": "0b9d04b0-7d39-4627-ce6a-82fd65a7d815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said - 414\n",
      "mrs - 289\n",
      "well - 229\n",
      "dont - 213\n",
      "aint - 187\n",
      "one - 174\n",
      "emarine - 172\n",
      "mother - 158\n",
      "go - 148\n",
      "oh - 146\n",
      "know - 143\n",
      "see - 142\n",
      "eyes - 133\n",
      "got - 129\n",
      "like - 127\n"
     ]
    }
   ],
   "source": [
    "# We still have some irrelevant words (dont aint go oh) => we sould remove short words like oh and go, and apply lemmatization\n",
    "N = 15\n",
    "for w, f in nltk.FreqDist(lws).most_common(N):\n",
    "    print(str(w) + ' - ' + str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGAW-wwj0mFo",
    "outputId": "4158b624-281a-4ade-b553-324553822edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said - 414\n",
      "mrs - 289\n",
      "well - 229\n",
      "dont - 213\n",
      "aint - 187\n",
      "one - 174\n",
      "emarine - 172\n",
      "mother - 158\n",
      "know - 143\n",
      "see - 142\n",
      "eyes - 133\n",
      "got - 129\n",
      "like - 127\n",
      "face - 125\n",
      "want - 115\n"
     ]
    }
   ],
   "source": [
    "# Results without short words (I think the results would be better with tokenization->punctuation removal->lemmatization->short words removal->numeral removal)\n",
    "rel_text = []\n",
    "for word in lws:\n",
    "    if len(word) > 2:\n",
    "        rel_text.append(word)\n",
    "N = 15\n",
    "for w, f in nltk.FreqDist(rel_text).most_common(N):\n",
    "    print(str(w) + ' - ' + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpv6FcvRzyym"
   },
   "source": [
    "# 8\n",
    "\n",
    "Apply stemming (Porter) on the list of words (lws). Print the first 200 words. Do you see any words that don't appear in the dictionary?\n",
    "\n",
    "https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUsDCm-3zw-P",
    "outputId": "6c936aae-5222-4e37-f475-ef2a2487962a"
   },
   "outputs": [],
   "source": [
    "porter_stemmer = nltk.PorterStemmer()\n",
    "stemms_porter = []\n",
    "\n",
    "for word in list(set(lws)):               # using set the get only the first appearance for word\n",
    "    stemms_porter.append(porter_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jexWnM0ODoOx"
   },
   "source": [
    "# 9\n",
    "\n",
    "Print a table of three columns (of size N, where N is the maximum length for the words in the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E49bAoRrDrDG",
    "outputId": "30b32ca8-3854-489e-8dc0-f4a376110e27"
   },
   "outputs": [],
   "source": [
    "lancaster_stemmer = nltk.LancasterStemmer()\n",
    "\n",
    "stemms_lancaster = []\n",
    "for word in list(set(lws)):\n",
    "    stemms_lancaster.append(lancaster_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuMfCUqMSVvc",
    "outputId": "31e67532-628e-4148-d295-da5956f1a253"
   },
   "outputs": [],
   "source": [
    "snowball_stemmer  = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "stemms_snowball = []\n",
    "for word in list(set(lws)):\n",
    "    stemms_snowball.append(snowball_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "II7JQKNjYIb5",
    "outputId": "ea7abe07-1a2a-47a2-d41e-5a32aab609c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6557\n",
      "6557\n",
      "6557\n"
     ]
    }
   ],
   "source": [
    "print(len(stemms_porter))\n",
    "print(len(stemms_lancaster))\n",
    "print(len(stemms_snowball))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qwzQVTTWawxg"
   },
   "outputs": [],
   "source": [
    "# I create a data frame containing the stemms for each stemmer\n",
    "\n",
    "df_stemms = pd.DataFrame({'porter': stemms_porter, 'lancaster': stemms_lancaster, 'snowball': stemms_snowball} ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BJCBXsTVbIzo"
   },
   "outputs": [],
   "source": [
    "# I select the ones with different results for each stemmer\n",
    "\n",
    "diff_porter = []\n",
    "diff_lancaster = [] \n",
    "diff_snowball = []\n",
    "\n",
    "for index, row in df_stemms.iterrows():\n",
    "    if row['porter'] != row['lancaster'] and  row['snowball'] != row['porter'] and row['snowball'] != row['lancaster']:\n",
    "        diff_porter.append(row['porter'])\n",
    "        diff_lancaster.append(row['lancaster'])\n",
    "        diff_snowball.append(row['snowball'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMOU61YOjQ8T"
   },
   "source": [
    "According to the result, the snowball stemmer gave the best results. (1 4 5 8 12 13 14 15 16 17 18 20 21 24 25 31 32 34 36 37 39 44 45 47 53 55 56 - examples where snawball gave the best result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZmE8-fQgcB5",
    "outputId": "a0dfa0c3-dda6-414d-f375-3394890abf18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Porter      Lancaster       Snowball\n",
      "0           preciou          precy       precious\n",
      "1        childishli          child       childish\n",
      "2          shrewdli       shrewdly         shrewd\n",
      "3            gentli           gent          gentl\n",
      "4             statu           stat         status\n",
      "5            aqueou            aqu        aqueous\n",
      "6             famou            fam         famous\n",
      "7             circu           circ         circus\n",
      "8         defiantli           defy        defiant\n",
      "9         instantli           inst        instant\n",
      "10         strongli       strongly         strong\n",
      "11          cautiou          cauty       cautious\n",
      "12         motherli           moth         mother\n",
      "13           overli             ov           over\n",
      "14           seriou           sery        serious\n",
      "15           nervou           nerv        nervous\n",
      "16           furiou           fury        furious\n",
      "17     coquettishli         coquet     coquettish\n",
      "18      irregularli        irregul      irregular\n",
      "19         brokenli           brok         broken\n",
      "20          humanli            hum          human\n",
      "21  perpendicularli    perpendicul  perpendicular\n",
      "22       petulantli            pet          petul\n",
      "23          previou          prevy       previous\n",
      "24           quietu          quiet        quietus\n",
      "25           virtju          virtj        virtjus\n",
      "26           impetu          impet        impetus\n",
      "27           anxiou           anxy        anxious\n",
      "28        naturedli            nat          natur\n",
      "29         consciou         conscy      conscious\n",
      "30        hurriedli          hurry          hurri\n",
      "31         bitterli            bit         bitter\n",
      "32    goodhumoredli        goodhum      goodhumor\n",
      "33            gener            gen        generat\n",
      "34      ravishingli            rav         ravish\n",
      "35          eagerli            eag          eager\n",
      "36             alwu            alw          alwus\n",
      "37    spoke—faintli  spoke—faintly    spoke—faint\n",
      "38           evenli             ev           even\n",
      "39             piou            pio          pious\n",
      "40        girl—onli      girl—only        girl—on\n",
      "41     particularli       particul     particular\n",
      "42       pleasantli          pleas       pleasant\n",
      "43         suddenli            sud         sudden\n",
      "44         tenderli           tend         tender\n",
      "45             viru            vir          virus\n",
      "46            gener            gen        general\n",
      "47          greatli            gre          great\n",
      "48        springvil    springville     springvill\n",
      "49      cloths—onli    cloths—only      cloths—on\n",
      "50        foolishli           fool        foolish\n",
      "51             vitu            vit          vitus\n",
      "52            gener            gen        general\n",
      "53      luxuriantli         luxury         luxuri\n",
      "54        popularli          popul        popular\n",
      "55           variou           vary        various\n",
      "56          gorgeou           gorg       gorgeous\n"
     ]
    }
   ],
   "source": [
    "# I create a data frame with different results for each stemmer\n",
    "\n",
    "df_diff_stemms = pd.DataFrame({'Porter': diff_porter, 'Lancaster': diff_lancaster, 'Snowball': diff_snowball} ) \n",
    "print(df_diff_stemms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckEFxa7alNF3"
   },
   "source": [
    "# 10\n",
    "\n",
    "Print a table of two columns, simillar to the one above, that will compare the results of stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1jigcqUlQi3",
    "outputId": "cb0898b0-0cbe-40a7-e5c1-c77aa83581c2"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for word in list(set(lws)):\n",
    "    lemmas.append(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dkrLaXfYxuOX"
   },
   "outputs": [],
   "source": [
    "df_stem_lem = pd.DataFrame({'WordNetLemmatizer': lemmas, 'snowball': stemms_snowball}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_169CsLzoDT",
    "outputId": "249798da-8228-4b28-d19d-0744320874bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     WordNetLemmatizer    Snowball\n",
      "0            know—here    know—her\n",
      "1              trembly      trembl\n",
      "2              h—hours      h—hour\n",
      "3             terrible     terribl\n",
      "4             jingling       jingl\n",
      "...                ...         ...\n",
      "3178       honeysuckle  honeysuckl\n",
      "3179           located       locat\n",
      "3180       manytongued   manytongu\n",
      "3181       reluctantly      reluct\n",
      "3182         smoothing      smooth\n",
      "\n",
      "[3183 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "diff_lemmas = []\n",
    "diff_snowball = []\n",
    "\n",
    "for index, row in df_stem_lem.iterrows():\n",
    "    if row['snowball'] != row['WordNetLemmatizer']:\n",
    "        diff_lemmas.append(row['WordNetLemmatizer'])\n",
    "        diff_snowball.append(row['snowball'])\n",
    "\n",
    "df_diff = pd.DataFrame({'WordNetLemmatizer': diff_lemmas, 'Snowball': diff_snowball})\n",
    "\n",
    "print(df_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdtGK8ag2gnB"
   },
   "source": [
    "# 11 \n",
    "Print the first N most frequent lemmas (after the removal of stopwords) together with their number of appearances.\n",
    "\n",
    "https://stackoverflow.com/a/5239831/12299607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ep2ruwM2ki7"
   },
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "for word in lws:\n",
    "    lemmas.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgWD7Nt05jNP",
    "outputId": "91b9dcd4-76aa-464b-e750-39c35b30e56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 414), ('mr', 375), ('well', 230), ('dont', 213), ('aint', 187), ('one', 186), ('mother', 180), ('emarine', 172), ('know', 157), ('go', 156)]\n"
     ]
    }
   ],
   "source": [
    "most_common_lemmas = collections.Counter(lemmas)\n",
    "print(most_common_lemmas.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxAtDmCl5-M7"
   },
   "source": [
    "# 12\n",
    "\n",
    "Change all the numbers from lws into words. Print the number of changes, and also the portion of list that contains first N changes (for example N=10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "q5jsV2fXOSac"
   },
   "outputs": [],
   "source": [
    "def has_numbers(str_):\n",
    "    ch = []\n",
    "    for c in str_:\n",
    "        if c.isdigit():\n",
    "            ch.append(c)\n",
    "    if ch != []:\n",
    "        return int(''.join(map(str, ch))) \n",
    "    else:\n",
    "        return str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1yKABgiOYmQ",
    "outputId": "0ce2fc32-3fcc-4806-ab04-6484f4f3fd0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "121\n",
      "mo\n"
     ]
    }
   ],
   "source": [
    "c = has_numbers('12mo')\n",
    "print(c)\n",
    "\n",
    "c = has_numbers('12mo1')\n",
    "print(c)\n",
    "\n",
    "c = has_numbers('mo')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY24P1uI6ETG",
    "outputId": "f5ac1049-7a6a-452c-cb90-808680424763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The portion of list that contains the first 2 changes\n",
      "Number of changes: 86\n"
     ]
    }
   ],
   "source": [
    "# The task done on the text\n",
    "\n",
    "N = 2\n",
    "contor = 0\n",
    "\n",
    "for idx, word in enumerate(lws):\n",
    "    c = has_numbers(word)             # Check if the element has numbers\n",
    "    \n",
    "    if isinstance(c, numbers.Number):\n",
    "        contor = contor + 1\n",
    "        lws[idx] = num2words(c)\n",
    "        \n",
    "        if contor == N:\n",
    "            print('The portion of list that contains the first ' + str(N) + ' changes')\n",
    "            #print(lws[:idx+1]) \n",
    "\n",
    "print('Number of changes:', contor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoW7jHrJRjt4"
   },
   "source": [
    "# 13\n",
    "\n",
    "Create a function that receives an integer N and a word W as parameter. We want to print the concordance data for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ajqNB234WrlR"
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "    data1 = data.translate(translate_table)\n",
    "    data2 = word_tokenize(data1)\n",
    "\n",
    "    data2 = [w.lower() for w in data2]\n",
    "\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    preprocessed_data = [x for x in data2 if not x in stopwords]  \n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RLaPy9_NRsYA"
   },
   "outputs": [],
   "source": [
    "def concordance(N, word, text):\n",
    "\n",
    "    data = preprocessing(text)\n",
    "    for id, w in enumerate(data):\n",
    "        output = []\n",
    "        if w == word:\n",
    "            output = data[id-math.floor(N/2) : id+math.floor(N/2)+1]\n",
    "            if len(output) >= N:\n",
    "                print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJ1RS78FTXuk",
    "outputId": "160b85b2-828c-4c7d-b0a6-3e5a52326d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'cat', 'pets']\n",
      "['pets', 'cat', 'likes']\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "W = 'cat'\n",
    "text = 'I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.'\n",
    "\n",
    "concordance(N, W, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c6QiLStbapHL"
   },
   "outputs": [],
   "source": [
    "# I apply the concordance function for each sentence,\n",
    "# Thus, the output words would be from the same sentence\n",
    "\n",
    "def concordance_same_phrase(N, W, T):\n",
    "    phrases = sent_tokenize(T)\n",
    "    for phrase in phrases:\n",
    "        concordance(N, W, phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuUgCKzHbSLL",
    "outputId": "6123928c-2b9c-4df2-df8f-0b8b9f4e1d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'cat', 'horse']\n"
     ]
    }
   ],
   "source": [
    "# For the example provided the output was empty, \n",
    "# so I added the horse to show that the function works properly \n",
    "\n",
    "N = 3\n",
    "W = 'cat'\n",
    "text = 'I have two dogs and a cat and a horse. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.'\n",
    "\n",
    "concordance_same_phrase(N, W, text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MajBtM9rgLpC",
    "Q5M75PnHUilo",
    "i0-zUQHHUqpO",
    "87VwImaJUtkS",
    "TmYrVVImWvLb",
    "fVS4wyJ0kW6M",
    "oPXpwy0xl0Dd",
    "L8kqm1NOxkoi",
    "tpv6FcvRzyym",
    "jexWnM0ODoOx",
    "ckEFxa7alNF3",
    "zdtGK8ag2gnB",
    "XxAtDmCl5-M7"
   ],
   "name": "Tema1_Ghadamiyan_Lida_407.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
